* Introduction
The main goal of this code base is to deploy an onnx model for inference on a Rest API while implementing automatic batching.
This README file contains code for:
- Cargo.toml
- Dockerfile
- Scripts for building and running the docker image.
For running the inference on both CPU and GPU.

* Main functions for processing the files
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  CLEAN(){
      rm -vf -- "./${1}"
  }

  GITCOMMIT () {
      git commit -m "${1}"
  }

  GITADD () {
      git add "./${1}"
  }

  SETFILE () {
      D="$('dirname' '--' "./${2}")"
      'mkdir' '-pv' '--' "${D}"
      'mv' '-vf' '--' "${1}" "${2}"
      'cargo' 'fmt'
      GITADD "${2}"
  }
#+end_src

* Clean files
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  CLEAN '#Cargo.toml#'
  CLEAN '.git.sh'
  CLEAN 'indent.log'
  CLEAN 'MODULAR.org~'
  CLEAN 'README.org~'
  CLEAN '#shell.nix#'
  CLEAN 'shell.nix~'
  CLEAN 'src/#main.rs#'
  CLEAN 'src/.#main.rs'
  CLEAN 'TEMPLATED.org~'
  CLEAN 'tmp.sh'
  CLEAN 'TRUE_QUEUE.org~'
#+end_src

* Add all the org files
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'FILE_BASED.org'
  GITADD 'MODULAR.org'
  GITADD 'README.org'
  GITADD 'TEMPLATED.org'
  GITADD 'TRUE_QUEUE.org'
#+end_src

* Main rust code

** For server

*** Move the temporary file and add it to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  SETFILE 'src,main.rs' 'src/main.rs'
#+end_src

*** Create the temporary file
#+begin_src rust :tangle ./src,main.rs
#+end_src

** For client

*** Move the temporary file and add it to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  SETFILE 'src,client.rs' 'src/client.rs'
#+end_src

*** Create the temporary file
#+begin_src rust :tangle ./src,client.rs
#+end_src

** Common lib

*** Move the temporary file and add it to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  SETFILE 'src,mylib.rs' 'src/mylib.rs'
#+end_src

*** Create the temporary file
#+begin_src rust :tangle ./src,mylib.rs
#+end_src

** Nodel execution parts

*** Move the temporary file and add it to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  SETFILE 'src,model.rs' 'src/model.rs'
#+end_src

*** Create the temporary file
#+begin_src rust :tangle ./src,model.rs
#+end_src

* Main cargo code

** Unify the file, clean temporaries and add it to git

*** Prepare the cargo file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  echo 'Start unifying cargo files' \
      && cat \
              './Cargo.package.toml' \
              './Cargo.dependencies.toml' \
              './Cargo.build-dependencies.toml' \
          > './Cargo.toml' \
  && echo 'Done unifying cargo files' ;
#+end_src

*** Clean temp files
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  CLEAN 'Cargo.build-dependencies.toml'
  CLEAN 'Cargo.dependencies.toml'
  CLEAN 'Cargo.package.toml'
#+end_src

*** Add the files to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'Cargo.toml'
  GITADD 'Cargo.lock'
#+end_src

** The cargo files

*** package
#+begin_src conf :tangle ./Cargo.package.toml
#+end_src

*** build-dependencies
#+begin_src conf :tangle ./Cargo.build-dependencies.toml
#+end_src

*** dependencies
#+begin_src conf :tangle ./Cargo.dependencies.toml
#+end_src

* Main nix code

** Format the nix code
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  'fd' '\.nix$' '.' '-t' 'f' '-x' 'alejandra' '{}'
#+end_src

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'shell.nix'
#+end_src

** Main nix code

*** Function inputs
#+begin_src nix :tangle ./shell.nix
  {pkgs ? import <nixpkgs> {}} :
#+end_src

*** Start convenience definitions

**** begin
#+begin_src nix :tangle ./shell.nix
  let
#+end_src

***** Package list

****** begin
#+begin_src nix :tangle ./shell.nix
  mylist = with pkgs; [
#+end_src

****** main

******* generic packages
#+begin_src nix :tangle ./shell.nix
  bc
  bison
  blend2d
  cargo
  cargo-info
  ffmpeg
  ffmpeg.dev
  fish
  flex
  fontconfig
  fontconfig.dev
  fontconfig.lib
  gnumake
  grpc-tools
  libelf
  nasm
  openssl
  openssl.dev
  pkg-config
  protobuf
  python313Full
  udev
  zsh
  zstd
#+end_src

****** end
#+begin_src nix :tangle ./shell.nix
  ] ;
#+end_src

**** end
#+begin_src nix :tangle ./shell.nix
  in
#+end_src

*** Function outputs for regular shell

**** Header
#+begin_src nix :tangle ./shell.nix
  (pkgs.mkShell {
#+end_src

***** Name
#+begin_src nix :tangle ./shell.nix
  name = "good_rust_env";
#+end_src

***** Packages
#+begin_src nix :tangle ./shell.nix
  packages = mylist;
#+end_src

***** Main shell command
#+begin_src nix :tangle ./shell.nix
  runScript = "fish";
#+end_src

**** Trailer
#+begin_src nix :tangle ./shell.nix
  })
#+end_src

* Build file

** Add to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'build.rs'
#+end_src

** Actual file
#+begin_src rust :tangle ./build.rs
  fn main() -> Result<(), Box<dyn std::error::Error>> {
      tonic_prost_build::compile_protos("./infer.proto")?;
      Ok(())
  }
#+end_src

* Define the image name

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'image_name.txt'
#+end_src

** Actual file having the name
#+begin_src conf :tangle ./image_name.txt
  onnxrust
#+end_src

* GIT Ignore stuff

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD '.gitignore'
#+end_src

** Actual file
#+begin_src conf :tangle ./.gitignore
  /image.jpg
  /image.png
  /IMAGES/
  /infer2.sh
  /model.onnx
  /target/
  /tmp/
  /proto/
#+end_src

* proto file

** Add the file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'infer.proto'
#+end_src

** Actual file
#+begin_src rust :tangle ./infer.proto
  syntax = "proto3";

  package infer;

  message Image {
      bytes image_data = 1;
  }

  message Prediction {
      float ps1 = 1;
      float ps2 = 2;
      float ps3 = 3;
  }

  service Infer {
    rpc doInfer(Image) returns (Prediction) {}
  }
#+end_src

* Prepare the docker build script

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'host.docker_build.sh'
#+end_src

** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
#+end_src

* Prepare the Dockerfile

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'Dockerfile'
#+end_src

** Create the file
#+begin_src conf :tangle ./Dockerfile
#+end_src

* Prepare the ROCM setup scripts

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
GITADD 'setup_rocm_2.sh'
GITADD 'setup_rocm_1.sh'
#+end_src

** Create the file

*** 1
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./setup_rocm_1.sh
#+end_src

*** 2
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./setup_rocm_2.sh
#+end_src

* Script to start server

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'start.sh'
#+end_src

** Actual file

*** header
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./start.sh
  cd "$(dirname -- "${0}")"
  export RUSTFLAGS="-C target-cpu=native"
#+end_src

*** start server and test grpc inference
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./start.sh
  cargo run --release --bin 'infer-server' &
  sleep 20 ; echo running inference ; cargo run --release --bin 'infer-client'
#+end_src


*** only start server
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./start.sh
  cargo run --release --bin 'infer-server'
#+end_src

*** trailer
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./start.sh
  echo done inference
  exit '0'
#+end_src

* Script to infer

** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD 'infer.sh'
#+end_src

** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./infer.sh
  curl -X POST "http://127.0.0.1:8000/infer" -F "file=@./image.png"
  curl -X POST "http://127.0.0.1:8000/infer" -F "file=@./image.jpg"
#+end_src

* General dependencies

** Cargo

*** package

**** Details
#+begin_src conf :tangle ./Cargo.package.toml
  [package]
  name = "onnxdeploy"
  version = "0.1.0"
  edition = "2024"
#+end_src

**** binary files

***** infer-server
#+begin_src conf :tangle ./Cargo.package.toml
  [[bin]]
  name = "infer-server"
  path = "src/main.rs"
#+end_src

***** infer-client
#+begin_src conf :tangle ./Cargo.package.toml
  [[bin]]
  name = "infer-client"
  path = "src/client.rs"
#+end_src

*** build-dependencies
#+begin_src conf :tangle ./Cargo.build-dependencies.toml
  [build-dependencies]
  tonic-prost-build = "0.14.2"
#+end_src

*** dependencies
#+begin_src conf :tangle ./Cargo.dependencies.toml
  [dependencies]
  actix-multipart = "0.7.2"
  actix-web = "4.11.0"
  bincode = { version = "2.0.1", features = ["serde"] }
  env_logger = "0.11.8"
  futures = "0.3.31"
  futures-util = "0.3.31"
  gxhash = "3.5.0"
  image = { version = "0.25.6", features = ["serde", "nasm"] }
  lockfree = "0.5.1"
  log = "0.4.27"
  ndarray = { version = "0.16.1", features = ["blas", "matrixmultiply-threading", "rayon", "serde"] }
  prost = "0.14"
  serde = { version = "1.0.219", features = ["derive"] }
  thiserror = "2.0.15"
  tokio = { version = "1.47.1", features = ["full"] }
  tonic-prost = "*"
  tonic = { version = "0.14.2", features = ["zstd"] }
#+end_src

* ORT Related
- Define ORT dependencies and features for GPU (CUDA) or CPU (OpenVino).
- Define docker base image for GPU or CPU.
- Definne nvidia gpu capabilities if using CUDA.
- Set env =export ORT_DYLIB_PATH='/lib/libonnxruntime.so.1'=

export ORT_DYLIB_PATH='/lib/libonnxruntime.so.1'
export ORT_STRATEGY='system'

** ROCM

*** Cargo
#+begin_src conf :tangle ./Cargo.dependencies.toml
  ort = { version = "2.0.0-rc.10", features = ["rocm", "load-dynamic"] }
#+end_src

*** Base docker image
#+begin_src conf :tangle ./Dockerfile
  # FROM rocm/onnxruntime:rocm7.0_ub24.04_ort1.22_torch2.8.0 AS rust
  # FROM rocm/dev-ubuntu-24.04:7.0-complete AS rust
  # FROM rocm/pytorch:latest AS rust

  FROM debian:bookworm-backports
#+end_src

*** Important pre requisits
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'environment-modules' \
          'python3-setuptools' \
          'python3-wheel' \
          'wget' \
      && echo 'DONE apt-get stuff' ;
#+end_src

*** ROCM Setup scripts

**** 1

***** Actual script
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./setup_rocm_1.sh
  echo 'START ROCM GPG' \
  && mkdir \
      --parents \
      --mode=0755 \
      '/etc/apt/keyrings' \
  && wget 'https://repo.radeon.com/rocm/rocm.gpg.key' -O - \
      | gpg '--dearmor' \
      | tee '/etc/apt/keyrings/rocm.gpg' \
  && echo 'DONE ROCM GPG' ;
#+end_src

***** Run the script
#+begin_src conf :tangle ./Dockerfile
  COPY ./setup_rocm_1.sh /root/setup_rocm_1.sh
  RUN /root/setup_rocm_1.sh
#+end_src

**** 2

***** Actual script
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./setup_rocm_2.sh
  echo 'START Update apt files' \
  && echo 'deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/rocm/apt/7.0.1 jammy main' > '/etc/apt/sources.list.d/rocm.list' \
  && echo 'deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/graphics/7.0.1/ubuntu jammy main' >> '/etc/apt/sources.list.d/rocm.list' \
  && echo 'Package: *' > '/etc/apt/preferences.d/rocm-pin-600' \
  && echo 'Pin: release o=repo.radeon.com' >> '/etc/apt/preferences.d/rocm-pin-600' \
  && echo 'Pin-Priority: 600' >> '/etc/apt/preferences.d/rocm-pin-600' \
  && echo 'DONE Update apt files' ;
#+end_src

***** Run the script
#+begin_src conf :tangle ./Dockerfile
  COPY ./setup_rocm_2.sh /root/setup_rocm_2.sh
  RUN /root/setup_rocm_2.sh
#+end_src

*** Important apt install stuff
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'rocm' \
      && echo 'DONE apt-get stuff' ;
#+end_src

Main link:
https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/install-methods/package-manager/package-manager-debian.html

*** Important apt install stuff
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'git' \
      && echo 'DONE apt-get stuff' ;
#+end_src

*** Clone onnx runtime
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'START clone onnx runtime' \
      && cd / \
      && git clone 'https://github.com/microsoft/onnxruntime.git' \
      && echo 'DONE clone onnx runtime' ;
#+end_src

*** Clone MIgraphx
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'START clone migraphx' \
      && cd "${HOME}" \
      && git clone 'https://github.com/ROCm/AMDMIGraphX.git' \
      && echo 'DONE migraphx' ;
#+end_src

*** Important apt install stuff
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'curl' \
      && echo 'DONE apt-get stuff' ;
#+end_src

*** Install uv
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'START uv download' \
      && curl -LsSf 'https://astral.sh/uv/install.sh' | sh \
      && cp -vf -- "${HOME}/.local/bin/uv" '/usr/local/bin/' \
      && echo 'DONE uv download' ;
#+end_src

*** Create env and install basic tools
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'START build and install onnxruntime' \
      && uv venv '/opt/venv' \
      && . '/opt/venv/bin/activate' \
      && uv pip install -U pip \
      && echo 'DONE build and install onnxruntime' ;
#+end_src

*** Get onnx rt requirements
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'START Get onnx rt requirements' \
      && . '/opt/venv/bin/activate' \
      && cd '/onnxruntime' \
      && uv pip install -r 'requirements-dev.txt' \
      && echo 'DONE Get onnx rt requirements' ;
#+end_src

*** Important apt install stuff
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'build-essential' \
          'cmake' \
          'gdb' \
          'git' \
          'half' \
          'migraphx' \
          'migraphx-dev' \
      && echo 'DONE apt-get stuff' ;
#+end_src

*** build and install onnxruntime
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'START build and install onnxruntime' \
      && cd "${HOME}/AMDMIGraphX" \
      && . '/opt/venv/bin/activate' \
      && './tools/build_and_test_onnxrt.sh' \
      ; echo 'DONE build and install onnxruntime' ;
#+end_src

*** Copy the installed libraries
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'START installed libraries' \
      && cd '/onnxruntime/build/Linux/Release' \
      && ls | grep 'lib.*\.so' | sed 's@^@("cp" "-vf" "--" "@g;s@$@" "/lib/");@g' | sh \
      && ldconfig \
      && echo 'DONE installed libraries' ;
#+end_src

*** Set up env
#+begin_src conf :tangle ./Dockerfile
  ENV ORT_DYLIB_PATH='/lib/libonnxruntime.so.1'
  ENV ORT_STRATEGY='system'
  ENV CC='/opt/rocm/llvm/bin/clang'
  ENV CXX='/opt/rocm/llvm/bin/clang++'
  ENV CMAKE_HIP_COMPILER='/opt/rocm/llvm/bin/clang++'
  ENV HIP_COMPILER='/opt/rocm/llvm/bin/clang++'
  ENV ORT_MIGRAPHX_SAVE_COMPILED_PATH='/COMPILED'
  ENV ORT_MIGRAPHX_LOAD_COMPILED_PATH="${ORT_MIGRAPHX_SAVE_COMPILED_PATH}"
  RUN mkdir -pv -- "${ORT_MIGRAPHX_SAVE_COMPILED_PATH}"
#+end_src

** COMMENT CUDA

*** Cargo
#+begin_src conf :tangle ./Cargo.dependencies.toml
  ort = { version = "2.0.0-rc.10", features = ["cuda"] }
#+end_src

*** Base docker image
#+begin_src conf :tangle ./Dockerfile
  FROM nvidia/cuda:12.9.1-cudnn-devel-ubuntu24.04 AS rust
#+end_src

*** env
#+begin_src conf :tangle ./Dockerfile
  ENV NVIDIA_DRIVER_CAPABILITIES='compute,utility,video'
#+end_src

** COMMENT WebGPU

*** Cargo
#+begin_src conf :tangle ./Cargo.dependencies.toml
  ort = { version = "2.0.0-rc.10", features = ["webgpu"] }
#+end_src

*** Base docker image
#+begin_src conf :tangle ./Dockerfile
  FROM ubuntu:24.04 AS rust
#+end_src

** COMMENT OpenVino

*** Cargo
#+begin_src conf :tangle ./Cargo.dependencies.toml
  ort = { version = "2.0.0-rc.10", features = ["openvino"] }
#+end_src

*** Base docker image
#+begin_src conf :tangle ./Dockerfile
  FROM openvino/ubuntu24_dev:latest AS rust
#+end_src

* Basic configs

** User and working dir
#+begin_src conf :tangle ./Dockerfile
  USER root
  WORKDIR '/root'
#+end_src

** Define important environment variables and working dir for apt
#+begin_src conf :tangle ./Dockerfile
  ENV HOME='/root'
  ENV DEBIAN_FRONTEND='noninteractive'
  ENV RUSTUP_HOME='/usr/local/rustup'
  ENV CARGO_HOME='/usr/local/cargo'
  ENV RUST_VERSION='1.90.0'
  ENV PATH="/usr/local/cargo/bin:${PATH}"
#+end_src

* Prepare basic packages

** Important apt install stuff
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'aria2' \
          'build-essential' \
          'cmake' \
          'curl' \
          'git' \
          'git-lfs' \
          'libfontconfig-dev' \
          'libssl-dev' \
          'make' \
          'nasm' \
          'pkg-config' \
          'wget' \
      && echo 'DONE apt-get stuff' ;
#+end_src

** Download rust 
Downloaad and install rust. Code taken from https://github.com/rust-lang/docker-rust
#+begin_src conf :tangle ./Dockerfile
  RUN set -eux; \
      dpkgArch="$(dpkg --print-architecture)"; \
      case "${dpkgArch##*-}" in \
          amd64) rustArch='x86_64-unknown-linux-gnu'; rustupSha256='20a06e644b0d9bd2fbdbfd52d42540bdde820ea7df86e92e533c073da0cdd43c' ;; \
          armhf) rustArch='armv7-unknown-linux-gnueabihf'; rustupSha256='3b8daab6cc3135f2cd4b12919559e6adaee73a2fbefb830fadf0405c20231d61' ;; \
          arm64) rustArch='aarch64-unknown-linux-gnu'; rustupSha256='e3853c5a252fca15252d07cb23a1bdd9377a8c6f3efa01531109281ae47f841c' ;; \
          i386) rustArch='i686-unknown-linux-gnu'; rustupSha256='a5db2c4b29d23e9b318b955dd0337d6b52e93933608469085c924e0d05b1df1f' ;; \
          ppc64el) rustArch='powerpc64le-unknown-linux-gnu'; rustupSha256='acd89c42b47c93bd4266163a7b05d3f26287d5148413c0d47b2e8a7aa67c9dc0' ;; \
          s390x) rustArch='s390x-unknown-linux-gnu'; rustupSha256='726b7fd5d8805e73eab4a024a2889f8859d5a44e36041abac0a2436a52d42572' ;; \
          riscv64) rustArch='riscv64gc-unknown-linux-gnu'; rustupSha256='09e64cc1b7a3e99adaa15dd2d46a3aad9d44d71041e2a96100d165c98a8fd7a7' ;; \
          ,*) echo >&2 "unsupported architecture: ${dpkgArch}"; exit 1 ;; \
      esac; \
      url="https://static.rust-lang.org/rustup/archive/1.28.2/${rustArch}/rustup-init"; \
      wget "$url"; \
      echo "${rustupSha256} *rustup-init" | sha256sum -c -; \
      chmod +x rustup-init; \
      ./rustup-init -y --no-modify-path --profile minimal --default-toolchain $RUST_VERSION --default-host ${rustArch}; \
      rm rustup-init; \
      chmod -R a+w $RUSTUP_HOME $CARGO_HOME; \
      rustup --version; \
      cargo --version; \
      rustc --version;
#+end_src

* Prepare with base system packages for rust
Build the main image

** Base image
#+begin_src conf :tangle ./Dockerfile
  FROM rust
#+end_src

** Important apt install stuff
Install the remaining apt packages
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'aria2' \
          'build-essential' \
          'cmake' \
          'curl' \
          'ffmpeg' \
          'fish' \
          'git' \
          'git-lfs' \
          'ipython3' \
          'libcairo2-dev' \
          'libfontconfig-dev' \
          'libopenblas64-dev' \
          'libopenblas-dev' \
          'libprotobuf-dev' \
          'libssl-dev' \
          'make' \
          'nasm' \
          'neovim' \
          'ninja-build' \
          'pkg-config' \
          'protobuf-compiler' \
          'python3-cairo-dev' \
          'python3-dev' \
          'python3-opencv' \
          'python3-pip' \
          'python3-setuptools' \
          'unzip' \
          'wget' \
      && echo 'DONE apt-get stuff' ;
#+end_src

* Expose a network port
Port on which the rest api listens to
#+begin_src conf :tangle ./Dockerfile
  EXPOSE 8000/tcp
#+end_src

* Script to run the docker image

** Main template

*** Add the file to git
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD host.docker_run.txt
#+end_src

*** Change dir
#+begin_src conf :tangle ./host.docker_run.txt
  cd "$('dirname' -- "${0}")" ;
#+end_src

*** Main command

**** docker
#+begin_src conf :tangle ./host.docker_run.txt
  sudo -A
  docker
#+end_src

**** COMMENT podman
#+begin_src conf :tangle ./host.docker_run.txt
  podman
#+end_src

*** run
#+begin_src conf :tangle ./host.docker_run.txt
  run
#+end_src

*** Interactive
#+begin_src conf :tangle ./host.docker_run.txt
  --tty
  --interactive
  --rm
#+end_src

*** GPU

**** COMMENT CUDA

***** COMMENT for all gpus
#+begin_src conf :tangle ./host.docker_run.txt
  --gpus 'all,"capabilities=compute,utility,video"'
#+end_src

***** For only 1 gpu
#+begin_src conf :tangle ./host.docker_run.txt
  --gpus 'device=0,"capabilities=compute,utility,video"'
#+end_src

***** IPC
#+begin_src conf :tangle ./host.docker_run.txt
  --ipc host
#+end_src

***** shm size
#+begin_src conf :tangle ./host.docker_run.txt
  --shm-size 107374182400
#+end_src

**** ROCM
#+begin_src conf :tangle ./host.docker_run.txt
  --device /dev/kfd
  --device /dev/dri
  --security-opt seccomp=unconfined
#+end_src

*** MOUNTS
#+begin_src conf :tangle ./host.docker_run.txt
  --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472'
  -v "$(realpath .):/data/input"
  -v "CACHE:/usr/local/cargo/registry"
  -v "CACHE:/root/.cache"
#+end_src

*** Network port
#+begin_src conf :tangle ./host.docker_run.txt
  -p '0.0.0.0:8000:8000/tcp'
#+end_src

*** memory size
#+begin_src conf :tangle ./host.docker_run.txt
  --ulimit memlock=-1
  --ulimit stack=67108864
#+end_src

*** Image name and command
#+begin_src conf :tangle ./host.docker_run.txt
  "$('cat' './image_name.txt')"
#+end_src

*** Final command

**** start the server
#+begin_src conf :tangle ./host.docker_run.txt
  '/data/input/start.sh' ;
#+end_src

**** COMMENT fish
#+begin_src conf :tangle ./host.docker_run.txt
  'fish' ;
#+end_src

** Prepare the main script from the template

*** Add the file to git 
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITADD host.docker_run.txt
#+end_src

*** Actual file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run.sh
  cd "$('dirname' -- "${0}")"
  cat './host.docker_run.txt' | tr '\n' ' ' > './host.docker_run_main.sh'
  sh './host.docker_run_main.sh'
#+end_src

* Script to build

** Change directory
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  cd "$('dirname' '--' "${0}")"
  IMAGE_NAME="$(cat './image_name.txt')"
#+end_src

** Actual build command

*** using docker
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  sudo -A docker build -t "${IMAGE_NAME}" .
#+end_src

*** COMMENT using podman
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  podman build -t "${IMAGE_NAME}" .
#+end_src

*** COMMENT using buildah
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  buildah build -t "${IMAGE_NAME}" .
#+end_src

* Code for creating the onnx session and image preprocessor

** Image processing

*** Import Image processing related modules
#+begin_src rust :tangle ./src,mylib.rs
  use image::DynamicImage;
  use image::imageops;
#+end_src

*** Struct for image processing

**** Data
#+begin_src rust :tangle ./src,mylib.rs
  pub struct image_processor {
      image_resolution: u32
  }
#+end_src

**** Methods
#+begin_src rust :tangle ./src,mylib.rs
  impl image_processor {
      pub fn new(val: u32) -> Self {
          return image_processor {
              image_resolution: val,
          };
      }
      fn preprocess(&self, img: DynamicImage) -> image::RgbaImage {
          let (width, height) = (img.width(), img.height());
          let size = width.min(height);
          let x = (width - size) / 2;
          let y = (height - size) / 2;
          let cropped_img = imageops::crop_imm(&img, x, y, size, size).to_image();
          imageops::resize(
              &cropped_img,
              self.image_resolution,
              self.image_resolution,
              imageops::FilterType::CatmullRom,
          )
      }
      pub fn decode_and_preprocess(&self, data: Vec<u8>) -> Result<image::RgbaImage, String> {
          match image::load_from_memory(&data) {
              Ok(img) => {
                  return Ok(self.preprocess(img));
              }
              Err(e) => {
                  return Err("decode error".to_string());
              }
          };
      }
  }
#+end_src

** Onnx session related

*** Import ort libraries
#+begin_src rust :tangle ./src,mylib.rs
  use ort::execution_providers::CUDAExecutionProvider;
  use ort::execution_providers::MIGraphXExecutionProvider;
  use ort::execution_providers::OpenVINOExecutionProvider;
  use ort::execution_providers::ROCmExecutionProvider;
  use ort::execution_providers::WebGPUExecutionProvider;
  use ort::session::Session;
  use ort::session::builder::GraphOptimizationLevel;
#+end_src

*** Functions to construct models

**** Main functions

***** Construct cuda model
#+begin_src rust :tangle ./src,mylib.rs
  pub fn get_cuda_model(model_path: &str) -> Result<Session, String> {
      let res1 = Session::builder()
          .unwrap()
          .with_optimization_level(GraphOptimizationLevel::Level3)
          .unwrap();

      let res2 = res1.with_execution_providers([CUDAExecutionProvider::default().build()]);

      match res2 {
          Ok(res3) => {
              let res4 = res3.commit_from_file(model_path).unwrap();
              println!("Constructed onnx with CUDA support");
              return Ok(res4);
          }
          Err(_) => {
              println!("Failed to construct model with CUDA support");
              return Err("Failed to construct model with CUDA support".to_string());
          }
      }
  }
#+end_src

***** Construct webgpu model
#+begin_src rust :tangle ./src,mylib.rs
  pub fn get_webgpu_model(model_path: &str) -> Result<Session, String> {
      let res1 = Session::builder()
          .unwrap()
          .with_optimization_level(GraphOptimizationLevel::Level3)
          .unwrap();

      let res2 = res1.with_execution_providers([WebGPUExecutionProvider::default().build()]);

      match res2 {
          Ok(res3) => {
              let res4 = res3.commit_from_file(model_path).unwrap();
              println!("Constructed onnx with CUDA support");
              return Ok(res4);
          }
          Err(_) => {
              println!("Failed to construct model with WebGPU support");
              return Err("Failed to construct model with WebGPU support".to_string());
          }
      }
  }
#+end_src

***** Construct openvino model
#+begin_src rust :tangle ./src,mylib.rs
  pub fn get_openvino_model(model_path: &str) -> Result<Session, String> {
      let res1 = Session::builder()
          .unwrap()
          .with_optimization_level(GraphOptimizationLevel::Level3)
          .unwrap();

      let res2 = res1.with_execution_providers([OpenVINOExecutionProvider::default().build()]);

      match res2 {
          Ok(res3) => {
              let res4 = res3.commit_from_file(model_path).unwrap();
              println!("Constructed onnx with openvino support");
              return Ok(res4);
          }
          Err(_) => {
              println!("Failed to construct model with openvino support");
              return Err("Failed to construct model with openvino support".to_string());
          }
      }
  }
#+end_src

***** Construct rocm model
#+begin_src rust :tangle ./src,mylib.rs
  pub fn get_rocm_model(model_path: &str) -> Result<Session, String> {
      let res1 = Session::builder()
          .unwrap()
          .with_optimization_level(GraphOptimizationLevel::Level3)
          .unwrap();

      let res2 = res1.with_execution_providers([MIGraphXExecutionProvider::default().build()]);

      match res2 {
          Ok(res3) => {
              let res4 = res3.commit_from_file(model_path).unwrap();
              println!("Constructed onnx with rocm support");
              return Ok(res4);
          }
          Err(_) => {
              println!("Failed to construct model with rocm support");
              return Err("Failed to construct model with rocm support".to_string());
          }
      }
  }
#+end_src

**** Wrapper function to construct the model
#+begin_src rust :tangle ./src,mylib.rs
  pub fn get_model(model_path: &str) -> Session {
      return get_rocm_model(model_path).unwrap();
  }
#+end_src

* Code for model related executions

** Important imports

*** ort parts
#+begin_src rust :tangle ./src,model.rs
  use ort::inputs;
  use ort::session::Session;
  use ort::value::TensorRef;
#+end_src

*** generic parts
#+begin_src rust :tangle ./src,model.rs
  use serde::Serialize;
#+end_src

*** std parts
#+begin_src rust :tangle ./src,model.rs
  use std::ops::Index;
  use std::time::Duration;
#+end_src

*** ndarray parts
#+begin_src rust :tangle ./src,model.rs
  use ndarray::Array;
  use ndarray::Axis;
  use ndarray::Ix4;
#+end_src

*** tokio part
#+begin_src rust :tangle ./src,model.rs
  use tokio;
  use tokio::sync::mpsc;
  use tokio::sync::oneshot;
#+end_src

*** My image processor and onnx model construction
#+begin_src rust :tangle ./src,model.rs
  use crate::mylib::get_model;
  use crate::mylib::image_processor;
#+end_src

** Important parameters

*** Generic
#+begin_src rust :tangle ./src,model.rs
  const MAX_BATCH: usize = 16;
  const BATCH_TIMEOUT: Duration = Duration::from_millis(200);
  const MODEL_PATH: &str = "./model.onnx";
#+end_src

*** Model specific
#+begin_src rust :tangle ./src,model.rs
  const IMAGE_RESOLUTION: u32 = 448;
  const CLASS_LABELS: [&str; num_features] = ["empty", "occupied", "other"];
#+end_src

*** Main output type
#+begin_src rust :tangle ./src,model.rs
  type outtype = f32;
#+end_src

*** Number of features in inferred results
#+begin_src rust :tangle ./src,model.rs
  const num_features: usize = 3;
#+end_src

** Structs for holding inference results

*** Main struct for holding inference results

**** Main struct
#+begin_src rust :tangle ./src,model.rs
  pub struct prediction_probabilities {
      pub ps: [outtype; num_features],
  }
#+end_src

**** Method implementation
#+begin_src rust :tangle ./src,model.rs
  impl prediction_probabilities {
      pub fn new() -> Self {
          prediction_probabilities {
              ps: [0.0; num_features],
          }
      }
      pub fn from<T: Index<usize, Output = outtype>>(input: T) -> Self {
          let mut ret = prediction_probabilities::new();
          for i in 0..num_features {
              ret.ps[i] = input[i];
          }
          return ret;
      }
  }
#+end_src

** Structure to construct the reply from server

*** Actual structure
#+begin_src rust :tangle ./src,model.rs
  #[derive(Serialize)]
  pub struct prediction_probabilities_reply {
      ps: [String; num_features],
      mj: String,
  }
#+end_src

*** Method implementation
#+begin_src rust :tangle ./src,model.rs
  impl prediction_probabilities_reply {
      pub fn new() -> Self {
          prediction_probabilities_reply {
              ps: std::array::from_fn(|_| String::new()),
              mj: String::new(),
          }
      }
      pub fn from(input: prediction_probabilities) -> prediction_probabilities_reply {
          let mut max_index: usize = 0;
          let mut ret = prediction_probabilities_reply::new();
          ret.ps[0] = input.ps[0].to_string();
          for i in 1..num_features {
              ret.ps[i] = input.ps[i].to_string();
              if input.ps[i] > input.ps[max_index] {
                  max_index = i;
              }
          }
          ret.mj = CLASS_LABELS[max_index].to_string() ;
          return ret;
      }
  }
#+end_src

** Struct for sending the inference request to the inferring thread
#+begin_src rust :tangle ./src,model.rs
  pub struct InferRequest {
      img: image::RgbaImage,
      resp_tx: oneshot::Sender<Result<prediction_probabilities, String>>,
  }
#+end_src

** implementing the inference with automatic batching

*** New struct based inference server

**** The struct
#+begin_src rust :tangle ./src,model.rs
  pub struct model_server {
      rx: mpsc::Receiver<InferRequest>,
      session: Session,
  }
#+end_src

**** The functions
#+begin_src rust :tangle ./src,model.rs
  impl model_server {
      pub async fn infer_loop(&mut self) {
          while let Some(first) = self.rx.recv().await {
              let mut batch = vec![first];
              let start = tokio::time::Instant::now();
              while batch.len() < MAX_BATCH && start.elapsed() < BATCH_TIMEOUT {
                  match self.rx.try_recv() {
                      Ok(req) => batch.push(req),
                      Err(_) => break,
                  }
              }
              let batch_size = batch.len();
              let mut input = Array::<u8, Ix4>::zeros((
                  batch_size,
                  IMAGE_RESOLUTION as usize,
                  IMAGE_RESOLUTION as usize,
                  3,
              ));
              for (i, req) in batch.iter().enumerate() {
                  for (x, y, pixel) in req.img.enumerate_pixels() {
                      let [r, g, b, _] = pixel.0;
                      input[[i, y as usize, x as usize, 0]] = r;
                      input[[i, y as usize, x as usize, 1]] = g;
                      input[[i, y as usize, x as usize, 2]] = b;
                  }
              }
              let outputs =
                  match self.session.run(inputs!["input" => TensorRef::from_array_view(&input).unwrap()]) {
                      Ok(o) => o,
                      Err(e) => {
                          for req in batch {
                              let _ = req.resp_tx.send(Err(format!("inference error: {}", e)));
                          }
                          continue;
                      }
                  };
              let output = outputs["output"]
                  .try_extract_array::<outtype>()
                  .unwrap()
                  .t()
                  .into_owned();
              for (row, req) in output.axis_iter(Axis(1)).zip(batch.into_iter()) {
                  let result = prediction_probabilities::from(row);
                  let _ = req.resp_tx.send(Ok(result));
              }
          }
      }
  }
#+end_src

** New struct based inference client

*** The struct
#+begin_src rust :tangle ./src,model.rs
  pub struct model_client {
      tx: mpsc::Sender<InferRequest>,
      preprocess: image_processor
  }
#+end_src

*** The functions
#+begin_src rust :tangle ./src,model.rs
  impl model_client {
      pub async fn do_infer(&self, img: image::RgbaImage) -> Result<prediction_probabilities, String> {
          let (resp_tx, resp_rx) = oneshot::channel();
          match self.tx.send(InferRequest { img, resp_tx }).await {
              Ok(_) => match resp_rx.await {
                  Ok(Ok(pred)) => {
                      return Ok(pred);
                  }
                  Ok(Err(e)) => {
                      return Err(e);
                  }
                  Err(e) => {
                      return Err("Recv Error".to_string());
                  }
              },
              Err(e) => {
                  return Err("Send error".to_string());
              }
          }
      }
      pub async fn do_infer_data(&self, data: Vec<u8>) -> Result<prediction_probabilities, String> {
          match self.preprocess.decode_and_preprocess(data) {
              Ok(img) => {
                  return self.do_infer(img).await;
              }
              Err(e) => {
                  return Err("Failed to decode and pre-process the image".to_string());
              }
          }
      }
  }
#+end_src

** Function to construct the inference server and client
#+begin_src rust :tangle ./src,model.rs
  pub fn get_inference_tuple() -> (model_server, model_client) {
      let (tx, rx) = mpsc::channel::<InferRequest>(512);
      let ret_server = model_server {
          rx: rx,
          session: get_model(MODEL_PATH),
      };
      let ret_client = model_client {tx: tx, preprocess: image_processor::new(IMAGE_RESOLUTION)};
      return (ret_server, ret_client);
  }
#+end_src

* Main code

** my modules

*** Declare mods
#+begin_src rust :tangle ./src,main.rs
  mod mylib;
  mod model;
#+end_src

*** Load structs and funcs from model module
#+begin_src rust :tangle ./src,main.rs
  use model::get_inference_tuple;
  use model::model_client;
  use model::prediction_probabilities_reply;
#+end_src

** Importing external libraries

*** tokio parts
#+begin_src rust :tangle ./src,main.rs
  use tokio;
#+end_src

*** generic parts
#+begin_src rust :tangle ./src,main.rs
  use futures_util::TryStreamExt;
#+end_src

*** std parts
#+begin_src rust :tangle ./src,main.rs
  use std::net::IpAddr;
  use std::net::Ipv4Addr;
  use std::net::SocketAddr;
  use std::sync::Arc;
#+end_src

** Main REST parts

*** import actix
#+begin_src rust :tangle ./src,main.rs
  use actix_multipart::Multipart;
  use actix_web::App;
  use actix_web::Error;
  use actix_web::HttpResponse;
  use actix_web::HttpServer;
  use actix_web::web;
#+end_src

*** Actual function called on api request
#+begin_src rust :tangle ./src,main.rs
  async fn infer_handler(
      mut payload: Multipart,
      infer_slave: web::Data<Arc<model_client>>,
  ) -> Result<HttpResponse, Error> {
      let mut data = Vec::new();
      while let Some(mut field) = payload.try_next().await? {
          while let Some(chunk) = field.try_next().await? {
              data.extend_from_slice(&chunk);
          }
      }
      if data.is_empty() {
          return Ok(HttpResponse::BadRequest().body("No image data"));
      }
      match infer_slave.do_infer_data(data).await {
          Ok(pred) => {
              return Ok(HttpResponse::Ok().json(prediction_probabilities_reply::from(pred)));
          },
          Err(e) => {
              return Ok(HttpResponse::InternalServerError().body(e));
          },
      }
  }
#+end_src

** Implementation for gRPC

*** Import

**** Main library
#+begin_src rust :tangle ./src,main.rs
  use tonic::Request;
  use tonic::Response;
  use tonic::Status;
#+end_src

**** proto parts

***** for server
#+begin_src rust :tangle ./src,main.rs
  pub mod infer {
      tonic::include_proto!("infer"); // The string specified here must match the proto package name
  }
#+end_src

***** for client
#+begin_src rust :tangle ./src,client.rs
  pub mod infer {
      tonic::include_proto!("infer"); // The string specified here must match the proto package name
  }
#+end_src

*** Main structure

**** Declare the structure
#+begin_src rust :tangle ./src,main.rs
  pub struct MyInferer {
      slave_client: Arc<model_client>
  }
#+end_src

**** The trait
#+begin_src rust :tangle ./src,main.rs
  #[tonic::async_trait]
  impl infer::infer_server::Infer for MyInferer {
      async fn do_infer(&self, request: Request<infer::Image>) -> Result<Response<infer::Prediction>, Status> {
          println!("Received gRPC request");
          let image_data = request.into_inner().image_data;
          match self.slave_client.do_infer_data(image_data).await {
              Ok(pred) => {
                  let reply = infer::Prediction {
                      ps1: pred.ps[0],
                      ps2: pred.ps[1],
                      ps3: pred.ps[2],
                  };
                  return Ok(Response::new(reply));
              },
              Err(e) => {
                  Err(Status::internal(e))
              },
          }
      }
  }
#+end_src

** The main function

*** Client

**** get important libraries
#+begin_src rust :tangle ./src,client.rs
  use std::fs;
  use std::error::Error;
#+end_src

**** define the main function
#+begin_src rust :tangle ./src,client.rs
  #[tokio::main]
  async fn main() -> Result<(), Box<dyn std::error::Error>> {
      let  data =  fs::read("./image.png").expect("Failed reading image file");
      let img = infer::Image{
          image_data: data
      };
      let mut client = infer::infer_client::InferClient::connect("http://127.0.0.1:8001").await?;
      let res = client.do_infer(img).await?;
      println!("{:?}",res);
      return Ok(());
  }
#+end_src

*** Server
#+begin_src rust :tangle ./src,main.rs
  #[actix_web::main]
  async fn main() -> () {
      let (mut slave_server, slave_client) = get_inference_tuple();
      let slave_client_1 = Arc::new(slave_client);
      let slave_client_2 = Arc::clone(&slave_client_1);
      let future_infer = slave_server.infer_loop();
      match HttpServer::new(move || {
          App::new()
              .app_data(web::Data::new(Arc::clone(&slave_client_1)))
              .route("/infer", web::post().to(infer_handler))
      })
      .bind(("0.0.0.0", 8000))
      {
          Ok(ret) => {
              let future_rest_server = ret.run();
              let ip_v4 = IpAddr::V4(Ipv4Addr::new(0, 0, 0, 0));
              let addr = SocketAddr::new(ip_v4, 8001);
              let inferer_service = MyInferer{slave_client: slave_client_2};
              let future_grpc = tonic::transport::Server::builder().add_service(infer::infer_server::InferServer::new(inferer_service)).serve(addr);
              let (first, second, third) = tokio::join!(future_infer, future_rest_server, future_grpc);
              match second {
                  Ok(_) => {
                      println!("REST server executed and stopped successfully");
                  }
                  Err(e) => {
                      println!("Encountered error in starting the server due to {}.", e);
                  }
              }
              match third {
                  Ok(_) => {
                      println!("GRPC server executed and stopped successfully");
                  }
                  Err(e) => {
                      println!("Encountered error in starting the server due to {}.", e);
                  }
              }
          }
          Err(e) => {
              eprintln!("Failed to bind to port");
          }
      }
  }
#+end_src

* COMMENT Pushing

** Just push
#+begin_src emacs-lisp :results silent
  (async-shell-command "
      git push
  " "log" "err")
#+end_src

** Prepare ssh key and push
#+begin_src emacs-lisp :results silent
  (async-shell-command "
      ~/SSH/KEYS/PERSONAL_LAPTOP_PERSONAL_GITHUB/setup.sh
      git push
  " "log" "err")
#+end_src

* Commit the changes
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  GITCOMMIT 'More migraphx debugging'
#+end_src

* Run the work script
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
          './.git.sh'
          git status
      " "log" "err")
#+end_src
