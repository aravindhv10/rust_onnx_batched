* COMMENT SAMPLE

** git worker
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
#+end_src

** nix
#+begin_src nix :tangle ./shell.nix
#+end_src

** Cargo
#+begin_src conf :tangle ./Cargo.toml
#+end_src

** Dockerfile
#+begin_src conf :tangle ./Dockerfile
#+end_src

** Script to build
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
#+end_src

** Script to run
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run.sh
#+end_src

** start
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./start.sh
#+end_src

** infer
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./infer.sh
#+end_src

** Text file to define docker commands
#+begin_src conf :tangle ./host.docker_run.txt
#+end_src

** Text file to define docker image name
#+begin_src conf :tangle ./image_name.txt
#+end_src

** Main rust code
#+begin_src rust :tangle ./src,main.rs
#+end_src

* Introduction
The main goal of this code base is to deploy an onnx model for inference on a Rest API while implementing automatic batching.
This README file contains code for:
- Cargo.toml
- Dockerfile
- Scripts for building and running the docker image.
For running the inference on both CPU and GPU.

* Define the image name
#+begin_src conf :tangle ./image_name.txt
  onnxrust
#+end_src

* General dependencies

** Cargo

*** Package details
#+begin_src conf :tangle ./Cargo.toml
  [package]
  name = "onnxdeploy"
  version = "0.1.0"
  edition = "2024"
#+end_src

*** Dependencies
#+begin_src conf :tangle ./Cargo.toml
  [dependencies]
  actix-multipart = "0.7.2"
  actix-web = "4.11.0"
  bincode = { version = "2.0.1", features = ["serde"] }
  env_logger = "0.11.8"
  futures = "0.3.31"
  futures-util = "0.3.31"
  gxhash = "3.5.0"
  image = { version = "0.25.6", features = ["serde", "nasm"] }
  lockfree = "0.5.1"
  log = "0.4.27"
  ndarray = { version = "0.16.1", features = ["blas", "matrixmultiply-threading", "rayon", "serde"] }
  serde = { version = "1.0.219", features = ["derive"] }
  thiserror = "2.0.15"
  tokio = { version = "1.47.1", features = ["full"] }
#+end_src

* ORT Related
- Define ORT dependencies and features for GPU (CUDA) or CPU (OpenVino).
- Define docker base image for GPU or CPU.
- Definne nvidia gpu capabilities if using CUDA.

** COMMENT CUDA

*** Cargo
#+begin_src conf :tangle ./Cargo.toml
  ort = { version = "2.0.0-rc.10", features = ["cuda"] }
#+end_src

*** Base docker image
#+begin_src conf :tangle ./Dockerfile
  FROM nvidia/cuda:12.9.1-cudnn-devel-ubuntu24.04 AS rust
#+end_src

*** env
#+begin_src conf :tangle ./Dockerfile
  ENV NVIDIA_DRIVER_CAPABILITIES='compute,utility,video'
#+end_src

** COMMENT WebGPU

*** Cargo
#+begin_src conf :tangle ./Cargo.toml
  ort = { version = "2.0.0-rc.10", features = ["webgpu"] }
#+end_src

*** Base docker image
#+begin_src conf :tangle ./Dockerfile
  FROM ubuntu:24.04 AS rust
#+end_src

** OpenVino

*** Cargo
#+begin_src conf :tangle ./Cargo.toml
  ort = { version = "2.0.0-rc.10", features = ["openvino"] }
#+end_src

*** Base docker image
#+begin_src conf :tangle ./Dockerfile
  FROM openvino/ubuntu24_dev:latest AS rust
#+end_src

* Basic configs
Define important environment variables and working dir for apt and rust.
#+begin_src conf :tangle ./Dockerfile
  ENV HOME='/root'
  ENV DEBIAN_FRONTEND='noninteractive'
  WORKDIR '/root'
  ENV RUSTUP_HOME=/usr/local/rustup \
      CARGO_HOME=/usr/local/cargo \
      PATH=/usr/local/cargo/bin:$PATH \
      RUST_VERSION=1.88.0
#+end_src

* Prepare basic packages

** Important apt install stuff
Install basic apt packages.
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'aria2' \
          'build-essential' \
          'cmake' \
          'curl' \
          'git' \
          'git-lfs' \
          'libfontconfig-dev' \
          'libssl-dev' \
          'make' \
          'nasm' \
          'pkg-config' \
          'wget' \
      && echo 'DONE apt-get stuff' ;
#+end_src

** Download rust 
Downloaad and install rust. Code taken from https://github.com/rust-lang/docker-rust
#+begin_src conf :tangle ./Dockerfile
  RUN set -eux; \
      dpkgArch="$(dpkg --print-architecture)"; \
      rustArch='x86_64-unknown-linux-gnu'; \
      rustupSha256='20a06e644b0d9bd2fbdbfd52d42540bdde820ea7df86e92e533c073da0cdd43c' ; \
      url="https://static.rust-lang.org/rustup/archive/1.28.2/${rustArch}/rustup-init"; \
      wget "$url"; \
      echo "${rustupSha256} *rustup-init" | sha256sum -c -; \
      chmod +x rustup-init; \
      ./rustup-init -y --no-modify-path --profile minimal --default-toolchain $RUST_VERSION --default-host ${rustArch}; \
      rm rustup-init; \
      chmod -R a+w $RUSTUP_HOME $CARGO_HOME; \
      rustup --version; \
      cargo --version; \
      rustc --version;
#+end_src

* Prepare with base system packages for rust
Build the main image

** Base image
#+begin_src conf :tangle ./Dockerfile
  FROM rust
#+end_src

** Important apt install stuff
Install the remaining apt packages
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'START apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          'aria2' \
          'build-essential' \
          'cmake' \
          'curl' \
          'ffmpeg' \
          'fish' \
          'git' \
          'git-lfs' \
          'ipython3' \
          'libcairo2-dev' \
          'libfontconfig-dev' \
          'libopenblas64-dev' \
          'libopenblas-dev' \
          'libssl-dev' \
          'make' \
          'nasm' \
          'neovim' \
          'ninja-build' \
          'pkg-config' \
          'python3-cairo-dev' \
          'python3-dev' \
          'python3-opencv' \
          'python3-pip' \
          'python3-setuptools' \
          'unzip' \
          'wget' \
      && echo 'DONE apt-get stuff' ;
#+end_src

* Expose a network port
Port on which the rest api listens to
#+begin_src conf :tangle ./Dockerfile
  EXPOSE 8000/tcp
#+end_src

* Script to run the docker image

** Main template

*** Change dir
#+begin_src conf :tangle ./host.docker_run.txt
  cd "$('dirname' -- "${0}")" ;
#+end_src

*** Main command

**** COMMENT docker
#+begin_src conf :tangle ./host.docker_run.txt
  sudo -A
  docker
#+end_src

**** podman
#+begin_src conf :tangle ./host.docker_run.txt
  podman
#+end_src

*** run
#+begin_src conf :tangle ./host.docker_run.txt
  run
#+end_src

*** Interactive
#+begin_src conf :tangle ./host.docker_run.txt
  --tty
  --interactive
  --rm
#+end_src

*** COMMENT CUDA
#+begin_src conf :tangle ./host.docker_run.txt
  --gpus 'all,"capabilities=compute,utility,video"'
#+end_src

*** IPC and shm sizes

**** IPC
#+begin_src conf :tangle ./host.docker_run.txt
  --ipc host
#+end_src

**** COMMENT shm size
#+begin_src conf :tangle ./host.docker_run.txt
  --shm-size 107374182400
#+end_src

*** MOUNTS
#+begin_src conf :tangle ./host.docker_run.txt
  --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472'
  -v "$(realpath .):/data/input"
  -v "CACHE:/usr/local/cargo/registry"
  -v "CACHE:/root/.cache"
#+end_src

*** Network port
#+begin_src conf :tangle ./host.docker_run.txt
  -p '0.0.0.0:8000:8000/tcp'
#+end_src

*** memory size
#+begin_src conf :tangle ./host.docker_run.txt
  --ulimit memlock=-1
  --ulimit stack=67108864
#+end_src

*** Image name and command
#+begin_src conf :tangle ./host.docker_run.txt
  "$('cat' './image_name.txt')"
#+end_src

*** Final command

**** start the server
#+begin_src conf :tangle ./host.docker_run.txt
  '/data/input/start.sh' ;
#+end_src

**** COMMENT fish
#+begin_src conf :tangle ./host.docker_run.txt
  'fish' ;
#+end_src

** Prepare the main script from the template
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run.sh
  cd "$('dirname' -- "${0}")"
  cat './host.docker_run.txt' | tr '\n' ' ' > './host.docker_run_main.sh'
  sh './host.docker_run_main.sh'
#+end_src

* Script to build

** Change directory
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  cd "$('dirname' '--' "${0}")"
#+end_src

** Actual build command

*** COMMENT using docker
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  sudo -A docker build -t onnxrust .
#+end_src

*** using podman
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  podman build -t "$('cat' './image_name.txt')" .
#+end_src

* Main nix shell code

** Main nix code

*** Function inputs
#+begin_src nix :tangle ./shell.nix
  {pkgs ? import <nixpkgs> {}} :
#+end_src

*** Start convenience definitions

**** begin
#+begin_src nix :tangle ./shell.nix
  let
#+end_src

***** Package list

****** begin
#+begin_src nix :tangle ./shell.nix
  mylist = with pkgs; [
#+end_src

****** main

******* generic packages
#+begin_src nix :tangle ./shell.nix
  bc
  bison
  blend2d
  cargo
  cargo-info
  ffmpeg
  ffmpeg.dev
  fish
  flex
  fontconfig
  fontconfig.dev
  fontconfig.lib
  gnumake
  libelf
  nasm
  openssl
  openssl.dev
  pkg-config
  python313Full
  udev
  zsh
  zstd
#+end_src

****** end
#+begin_src nix :tangle ./shell.nix
  ] ;
#+end_src

**** end
#+begin_src nix :tangle ./shell.nix
  in
#+end_src

*** Function outputs for regular shell

**** Header
#+begin_src nix :tangle ./shell.nix
  (pkgs.mkShell {
#+end_src

***** Name
#+begin_src nix :tangle ./shell.nix
  name = "good_rust_env";
#+end_src

***** Packages
#+begin_src nix :tangle ./shell.nix
  packages = mylist;
#+end_src

***** Main shell command
#+begin_src nix :tangle ./shell.nix
  runScript = "fish";
#+end_src

**** Trailer
#+begin_src nix :tangle ./shell.nix
  })
#+end_src

* Script to start server
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./start.sh
  cd "$(dirname -- "${0}")"
  export RUSTFLAGS="-C target-cpu=native"
  cargo run --release
#+end_src

* Script to infer
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./infer.sh
  curl -X POST "http://127.0.0.1:8000/infer" -F "file=@./image.png"
  curl -X POST "http://127.0.0.1:8000/infer" -F "file=@./image.jpg"
#+end_src

* Main code

** Importing external libraries

*** COMMENT Disabled parts
#+begin_src rust :tangle ./src,main.rs
  use bincode::Decode;
  use bincode::Encode;
  use bincode::config;
  use futures::future::join_all;
  use gxhash;
  use std::fs;
  use std::path::Path;
  use std::time::SystemTime;
  use tokio::fs::create_dir_all;
  use tokio::fs::read;
  use tokio::fs::read_dir;
  use tokio::fs::remove_file;
  use tokio::fs::write;
  use tokio::sync::Mutex;
#+end_src

*** Main parts
#+begin_src rust :tangle ./src,main.rs
  use actix_multipart::Multipart;
  use actix_web::App;
  use actix_web::Error;
  use actix_web::HttpResponse;
  use actix_web::HttpServer;
  use actix_web::web;
  use futures_util::TryStreamExt;
  use image::DynamicImage;
  use image::imageops;
  use ndarray::Array;
  use ndarray::Axis;
  use ndarray::Ix4;
  use ort::execution_providers::CUDAExecutionProvider;
  use ort::execution_providers::OpenVINOExecutionProvider;
  use ort::execution_providers::WebGPUExecutionProvider;
  use ort::inputs;
  use ort::session::Session;
  use ort::session::builder::GraphOptimizationLevel;
  use ort::value::TensorRef;
  use serde::Deserialize;
  use serde::Serialize;
  use std::ops::Index;
  use std::time::Duration;
  use tokio;
  use tokio::sync::mpsc;
  use tokio::sync::oneshot;
#+end_src

*** Important parameters

**** Generic
#+begin_src rust :tangle ./src,main.rs
  const MAX_BATCH: usize = 16;
  const BATCH_TIMEOUT: Duration = Duration::from_millis(20);
  const MODEL_PATH: &str = "./model.onnx";
#+end_src

**** Model specific
#+begin_src rust :tangle ./src,main.rs
  const IMAGE_RESOLUTION: u32 = 448;
  const num_features: usize = 3;
  const CLASS_LABELS: [&str; num_features] = ["empty", "occupied", "other"];
#+end_src

** Main struct for holding inference results

*** Main struct
#+begin_src rust :tangle ./src,main.rs
  struct prediction_probabilities {
      ps: [f32; num_features],
  }
#+end_src

*** Method implementation
#+begin_src rust :tangle ./src,main.rs
  impl prediction_probabilities {
      fn new() -> prediction_probabilities {
          prediction_probabilities {
              ps: [0.0; num_features],
          }
      }

      fn from<T: Index<usize, Output = f32>>(input: T) -> prediction_probabilities {
          let mut ret = prediction_probabilities::new();
          for i in 0..num_features {
              ret.ps[i] = input[i];
          }
          ret
      }
  }
#+end_src

** Structure to construct the reply from server

*** Actual structure
#+begin_src rust :tangle ./src,main.rs
  #[derive(Serialize)]
  struct prediction_probabilities_reply {
      ps: [String; num_features],
      mj: String,
  }
#+end_src

*** Method implementation
#+begin_src rust :tangle ./src,main.rs
  impl prediction_probabilities_reply {
      fn from(input: prediction_probabilities) -> prediction_probabilities_reply {
          let mut max_index: usize = 0;

          for i in 1..num_features {
              if input.ps[i] > input.ps[max_index] {
                  max_index = i;
              }
          }

          let ret = prediction_probabilities_reply {
              ps: [
                  input.ps[0].to_string(),
                  input.ps[1].to_string(),
                  input.ps[2].to_string(),
              ],
              mj: CLASS_LABELS[max_index].to_string(),
          };

          ret
      }
  }
#+end_src

** Struct for sending the inference request to the inferring thread
#+begin_src rust :tangle ./src,main.rs
  // === Request to inference thread ===
  struct InferRequest {
      img: DynamicImage,
      resp_tx: oneshot::Sender<Result<prediction_probabilities, String>>,
  }
#+end_src

** Function to preprocess the image (center cropping, resizing, ...)
#+begin_src rust :tangle ./src,main.rs
  fn preprocess(img: DynamicImage) -> image::RgbaImage {
      let (width, height) = (img.width(), img.height());
      let size = width.min(height);
      let x = (width - size) / 2;
      let y = (height - size) / 2;
      let cropped_img = imageops::crop_imm(&img, x, y, size, size).to_image();
      imageops::resize(
          &cropped_img,
          IMAGE_RESOLUTION,
          IMAGE_RESOLUTION,
          imageops::FilterType::CatmullRom,
      )
  }
#+end_src

** Main function which gets run by actix for inference
#+begin_src rust :tangle ./src,main.rs
  async fn infer_handler(
      mut payload: Multipart,
      tx: web::Data<mpsc::Sender<InferRequest>>,
  ) -> Result<HttpResponse, Error> {
      let mut data = Vec::new();
      while let Some(mut field) = payload.try_next().await? {
          while let Some(chunk) = field.try_next().await? {
              data.extend_from_slice(&chunk);
          }
      }
      if data.is_empty() {
          return Ok(HttpResponse::BadRequest().body("No image data"));
      }

      let img = image::load_from_memory(&data)
          .map_err(|e| actix_web::error::ErrorBadRequest(format!("decode error: {}", e)))?;

      let (resp_tx, resp_rx) = oneshot::channel();
      tx.send(InferRequest { img, resp_tx })
          .await
          .map_err(|_| actix_web::error::ErrorInternalServerError("inference queue closed"))?;

      match resp_rx.await {
          Ok(Ok(pred)) => Ok(HttpResponse::Ok().json(prediction_probabilities_reply::from(pred))),
          Ok(Err(e)) => Ok(HttpResponse::InternalServerError().body(e)),
          Err(_) => Ok(HttpResponse::InternalServerError().body("inference dropped")),
      }
  }
#+end_src

** Main function to run the inference loops
#+begin_src rust :tangle ./src,main.rs
  async fn infer_loop(mut rx: mpsc::Receiver<InferRequest>, mut session: Session) {
      while let Some(first) = rx.recv().await {
          let mut batch = vec![first];
          let start = tokio::time::Instant::now();
          while batch.len() < MAX_BATCH && start.elapsed() < BATCH_TIMEOUT {
              match rx.try_recv() {
                  Ok(req) => batch.push(req),
                  Err(_) => break,
              }
          }

          let batch_size = batch.len();
          let mut input = Array::<u8, Ix4>::zeros((
              batch_size,
              IMAGE_RESOLUTION as usize,
              IMAGE_RESOLUTION as usize,
              3,
          ));

          for (i, req) in batch.iter().enumerate() {
              let img = preprocess(req.img.clone());
              for (x, y, pixel) in img.enumerate_pixels() {
                  let [r, g, b, _] = pixel.0;
                  input[[i, y as usize, x as usize, 0]] = r;
                  input[[i, y as usize, x as usize, 1]] = g;
                  input[[i, y as usize, x as usize, 2]] = b;
              }
          }

          let outputs =
              match session.run(inputs!["input" => TensorRef::from_array_view(&input).unwrap()]) {
                  Ok(o) => o,
                  Err(e) => {
                      for req in batch {
                          let _ = req.resp_tx.send(Err(format!("inference error: {}", e)));
                      }
                      continue;
                  }
              };

          let output = outputs["output"]
              .try_extract_array::<f32>()
              .unwrap()
              .t()
              .into_owned();

          for (row, req) in output.axis_iter(Axis(1)).zip(batch.into_iter()) {
              let result = prediction_probabilities::from(row);
              let _ = req.resp_tx.send(Ok(result));
          }
      }
  }
#+end_src

** Function to construct the models

*** Main function to get the models for various execution providers

**** cuda
#+begin_src rust :tangle ./src,main.rs
  fn get_cuda_model() -> Result<Session, String> {
      let res1 = Session::builder()
          .unwrap()
          .with_optimization_level(GraphOptimizationLevel::Level3)
          .unwrap();

      let res2 = res1.with_execution_providers([CUDAExecutionProvider::default().build()]);

      match res2 {
          Ok(res3) => {
              let res4 = res3.commit_from_file(MODEL_PATH).unwrap();
              println!("Constructed onnx with CUDA support");
              return Ok(res4);
          }
          Err(_) => {
              println!("Failed to construct model with CUDA support");
              return Err("Failed to construct model with CUDA support".to_string());
          }
      }
  }
#+end_src

**** webgpu
#+begin_src rust :tangle ./src,main.rs
  fn get_webgpu_model() -> Result<Session, String> {
      let res1 = Session::builder()
          .unwrap()
          .with_optimization_level(GraphOptimizationLevel::Level3)
          .unwrap();

      let res2 = res1.with_execution_providers([WebGPUExecutionProvider::default().build()]);

      match res2 {
          Ok(res3) => {
              let res4 = res3.commit_from_file(MODEL_PATH).unwrap();
              println!("Constructed onnx with CUDA support");
              return Ok(res4);
          }
          Err(_) => {
              println!("Failed to construct model with WebGPU support");
              return Err("Failed to construct model with WebGPU support".to_string());
          }
      }
  }
#+end_src

**** openvino
#+begin_src rust :tangle ./src,main.rs
  fn get_openvino_model() -> Result<Session, String> {
      let res1 = Session::builder()
          .unwrap()
          .with_optimization_level(GraphOptimizationLevel::Level3)
          .unwrap();

      let res2 = res1.with_execution_providers([OpenVINOExecutionProvider::default().build()]);

      match res2 {
          Ok(res3) => {
              let res4 = res3.commit_from_file(MODEL_PATH).unwrap();
              println!("Constructed onnx with openvino support");
              return Ok(res4);
          }
          Err(_) => {
              println!("Failed to construct model with openvino support");
              return Err("Failed to construct model with openvino support".to_string());
          }
      }
  }
#+end_src

*** Wrapper function to get the model
#+begin_src rust :tangle ./src,main.rs
  fn get_model() -> Session {
      match get_cuda_model() {
          Ok(model) => {
              return model;
          }
          Err(_) => {
              return get_openvino_model().unwrap();
          }
      }
  }
#+end_src

** The main function
#+begin_src rust :tangle ./src,main.rs
  #[actix_web::main]
  async fn main() -> std::io::Result<()> {
      let model = get_model();
      let (tx, rx) = mpsc::channel::<InferRequest>(1000);

      tokio::spawn(infer_loop(rx, model));

      HttpServer::new(move || {
          App::new()
              .app_data(web::Data::new(tx.clone()))
              .route("/infer", web::post().to(infer_handler))
      })
      .bind(("0.0.0.0", 8000))?
      .run()
      .await
  }
#+end_src

* GIT Ignore stuff
#+begin_src conf :tangle ./.gitignore
  /image.jpg
  /image.png
  /IMAGES/
  /infer2.sh
  /model.onnx
  /target/
  /tmp/
#+end_src

* WORK SPACE

** git worker

*** Functions
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G () {
      git add "./${1}"
  }

  C(){
      rm -vf -- "./${1}"
  }

  M () {
      git commit -m "${1}"
  }
#+end_src

*** Prepare the main rust file
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  mkdir ./src/
  mv -vf -- ./src,main.rs ./src/main.rs
#+end_src

*** Add files
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  G 'Cargo.lock'
  G 'Cargo.toml'
  G 'Dockerfile'
  G 'FILE_BASED.org'
  G '.gitignore'
  G 'host.docker_build.sh'
  G 'host.docker_run_main.sh'
  G 'host.docker_run.sh'
  G 'host.docker_run.txt'
  G 'image_name.txt'
  G 'infer.sh'
  G 'README.org'
  G 'shell.nix'
  G 'src/main.rs'
  G 'start.sh'
  G 'TRUE_QUEUE.org'
#+end_src

*** Clean files
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  C '.git.sh'
  C 'indent.log'
  C 'README.org~'
  C '#shell.nix#'
  C 'shell.nix~'
  C 'src/#main.rs#'
  C 'src/.#main.rs'
  C 'tmp.sh'
  C 'TRUE_QUEUE.org~'
#+end_src

*** Commit the changes
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./.git.sh
  M 'WORKING: Better doc and streamlined code'
#+end_src

** COMMENT elisp
#+begin_src emacs-lisp :results silent
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "
          # find ./ -type f | grep '\.nix$' | sed 's@^@alejandra \"@g ; s@$@\"@g' | sh
          './.git.sh'
          git status
      " "log" "err")
#+end_src

** COMMENT Pushing

*** Prepare ssh key and push
#+begin_src emacs-lisp :results silent
  (async-shell-command "
      ~/SSH/KEYS/PERSONAL_LAPTOP_PERSONAL_GITHUB/setup.sh
      git push
  " "log" "err")
#+end_src

*** Just push
#+begin_src emacs-lisp :results silent
  (async-shell-command "
      git push
  " "log" "err")
#+end_src
